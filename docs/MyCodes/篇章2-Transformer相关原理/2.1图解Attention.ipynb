{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b1536d",
   "metadata": {},
   "source": [
    "问题：Attention出现的原因是什么？\n",
    "\n",
    "潜在的答案：基于循环神经网络（RNN）一类的seq2seq模型，在处理长文本时遇到了挑战，而对长文本中不同位置的信息进行attention有助于提升RNN的模型效果。\n",
    "\n",
    "于是学习的问题就拆解为：\n",
    "1. 什么是seq2seq模型？\n",
    "2. 基于RNN的seq2seq模型如何处理文本/长文本序列？\n",
    "3. seq2seq模型处理长文本序列时遇到了什么问题？\n",
    "4.基于RNN的seq2seq模型如何结合attention来改善模型效果？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd3231a",
   "metadata": {},
   "source": [
    "attention起到了什么作用？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8052a",
   "metadata": {},
   "source": [
    "## seq2seq框架\n",
    "\n",
    "seq2seq是一种常见的NLP模型结构，全称是：sequence to sequence，翻译为“序列到序列”。顾名思义：从一个文本序列得到一个新的文本序列。典型的任务有：机器翻译任务，文本摘要任务。谷歌翻译在2016年末开始使用seq2seq模型，并发表了2篇开创性的论文：[Sutskever等2014年发表的Sequence to Sequence Learning\n",
    "with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)和[Cho等2014年发表的Learning Phrase Representations using RNN Encoder–Decoder\n",
    "for Statistical Machine Translation](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf)，感兴趣的读者可以阅读原文进行学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d698cff",
   "metadata": {},
   "source": [
    "seq2seq模型中的编码器和解码器一般采用的是循环神经网络RNN（Transformer模型还没出现的过去时代）；\n",
    "\n",
    "输入——编码——context向量——解码——输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245076e",
   "metadata": {},
   "source": [
    "单词映射成为向量，通常使用 word embedding 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e2cb4",
   "metadata": {},
   "source": [
    "## Attention\n",
    "基于RNN的seq2seq模型编码器所有信息都编码到了一个context向量中，便是这类模型的瓶颈。一方面单个向量很难包含所有文本序列的信息，另一方面RNN递归地编码文本序列使得模型在处理长文本时面临非常大的挑战（比如RNN处理到第500个单词的时候，很难再包含1-499个单词中的所有信息了）。\n",
    "\n",
    "面对以上问题，Bahdanau等2014发布的[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) 和 Luong等2015年发布的[Effective Approaches to Attention-based Neural Machine Translation\n",
    "](https://arxiv.org/abs/1508.04025)两篇论文中，提出了一种叫做注意力**attetion**的技术。通过attention技术，seq2seq模型极大地提高了机器翻译的质量。归其原因是：attention注意力机制，使得seq2seq模型可以有区分度、有重点地关注输入序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de986578",
   "metadata": {},
   "source": [
    "让我们继续来理解带有注意力的seq2seq模型：一个注意力模型与经典的seq2seq模型主要有2点不同：\n",
    "\n",
    "\n",
    "- A. 首先，编码器会把更多的数据传递给解码器。编码器把所有时间步的 hidden state（隐藏层状态）传递给解码器，而不是只传递最后一个 hidden state（隐藏层状态），如下面的动态图所示:\n",
    "![](./pictures/1-6-mt-1.gif) 动态图: 更多的信息传递给decoder\n",
    "\n",
    "- B. 注意力模型的解码器在产生输出之前，做了一个额外的attention处理。如下图所示，具体为：\n",
    "\n",
    "  - 1. 由于编码器中每个 hidden state（隐藏层状态）都对应到输入句子中一个单词，那么解码器要查看所有接收到的编码器的 hidden state（隐藏层状态）。\n",
    "  - 2. 给每个 hidden state（隐藏层状态）计算出一个分数（我们先忽略这个分数的计算过程）。\n",
    "  - 3. 所有hidden state（隐藏层状态）的分数经过softmax进行归一化。\n",
    "  - 4. 将每个 hidden state（隐藏层状态）乘以所对应的分数，从而能够让高分对应的  hidden state（隐藏层状态）会被放大，而低分对应的  hidden state（隐藏层状态）会被缩小。\n",
    "  - 5. 将所有hidden state根据对应分数进行加权求和，得到对应时间步的context向量。\n",
    "  ![](./pictures/1-7-attention-dec.gif) 动态图：在第4个时间步，编码器结合attention得到context向量的5个步骤。\n",
    "\n",
    "所以，attention可以简单理解为：一种有效的加权求和技术，其艺术在于如何获得权重。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0830c915",
   "metadata": {},
   "source": [
    "注：理解——attention最主要的改进，首先是传入了编码器中所有的隐藏层状态。 其次是，解码器在不同时间步中，会根据隐藏状态计算分数，分数再sofamax归一化；(再用这个归一化之后的数字，乘以对应的隐藏层状态，再求和)加权求和。其实就是在这一层中，主要使用了前面传入的哪一层隐藏层状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aabab1",
   "metadata": {},
   "source": [
    "现在，让我们把所有内容都融合到下面的图中，来看看结合注意力的seq2seq模型解码器全流程，动态图展示的是第4个时间步：\n",
    "\n",
    "1. 注意力模型的解码器 RNN 的输入包括：一个word embedding 向量，和一个初始化好的解码器 hidden state，图中是$h_{init}$。\n",
    "2. RNN 处理上述的 2 个输入，产生一个输出和一个新的 hidden state，图中为h4。\n",
    "3. 注意力的步骤：我们使用编码器的所有 hidden state向量和 h4 向量来计算这个时间步的context向量（C4）。\n",
    "4. 我们把 h4 和 C4 拼接起来，得到一个橙色向量。\n",
    "5. 我们把这个橙色向量输入一个前馈神经网络（这个网络是和整个模型一起训练的）。\n",
    "6. 根据前馈神经网络的输出向量得到输出单词：假设输出序列可能的单词有N个，那么这个前馈神经网络的输出向量通常是N维的，每个维度的下标对应一个输出单词，每个维度的数值对应的是该单词的输出概率。\n",
    "7. 在下一个时间步重复1-6步骤。\n",
    "![](./pictures/1-7-attention-pro.gif) 动态图：解码器结合attention全过程\n",
    "\n",
    "到目前为止，希望你已经知道本文开头提出的3、4问题的答案啦：3、seq2seq处理长文本序列的挑战是什么？4、seq2seq是如何结合attention来解决问题3中的挑战的？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f8818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
