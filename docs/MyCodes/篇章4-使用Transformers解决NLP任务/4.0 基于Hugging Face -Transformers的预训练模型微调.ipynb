{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9ecd06a",
   "metadata": {},
   "source": [
    "本文参考资料是[Hugging Face主页](https://huggingface.co/)Resources下的课程,节选部分内容并注释（加粗斜体），也加了Trainer和args的主要参数介绍。感兴趣的同学可以去查看[原文](https://huggingface.co/course/chapter1)。\n",
    "****\n",
    "本章节主要内容包含两部分内容：\n",
    "- pipeline工具演示NLP任务处理\n",
    "- 构建Trainer微调模型<br>\n",
    "\n",
    "目录\n",
    "- [1.  简介](#1--简介)\n",
    "  - [Transformers的历史](#transformers的历史)\n",
    "  - [Architectures和checkpoints](#architectures和checkpoints)\n",
    "  - [The Inference API](#the-inference-api)\n",
    "- [2. 用pipeline处理NLP问题](#2-用pipeline处理nlp问题)\n",
    "- [3. Behind the pipeline](#3-behind-the-pipeline)\n",
    "  - [tokenizer预处理](#tokenizer预处理)\n",
    "  - [选择模型](#选择模型)\n",
    "  - [Model heads](#model-heads)\n",
    "  - [Post-processing后处理](#post-processing后处理)\n",
    "- [4. 构建Trainer API微调预训练模型](#4-构建trainer-api微调预训练模型)\n",
    "  - [从Hub上下载dataset](#从hub上下载dataset)\n",
    "  - [数据集预处理](#数据集预处理)\n",
    "  - [使用 Trainer API 在 PyTorch 中进行微调](#使用-trainer-api-在-pytorch-中进行微调)\n",
    "    - [训练](#训练)\n",
    "    - [评估函数](#评估函数)\n",
    "- [5. 补充部分](#5-补充部分)\n",
    "  - [为什么教程第四章都是用Trainer来微调模型？](#为什么教程第四章都是用trainer来微调模型)\n",
    "  - [TrainingArguments主要参数](#trainingarguments主要参数)\n",
    "  - [不同的模型加载方式](#不同的模型加载方式)\n",
    "  - [Dynamic padding——动态填充技术](#dynamic-padding动态填充技术)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd0c69d",
   "metadata": {},
   "source": [
    "## 1.  简介\n",
    "本章节将使用 [Hugging Face 生态系统中的库](https://github.com/huggingface)——🤗 Transformers来进行自然语言处理工作(NLP)。\n",
    "### Transformers的历史\n",
    "以下是 Transformer 模型（简短）历史中的一些参考点：\n",
    "![transformers_chrono](https://img-blog.csdnimg.cn/3ba51fe4f21d4d528ca7b0f2fd78aee4.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)\n",
    "[Transformer 架构](https://arxiv.org/abs/1706.03762)于 2017 年 6 月推出。原始研究的重点是翻译任务。随后推出了几个有影响力的模型，包括：\n",
    "\n",
    "- 2018 年 6 月：[GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)，第一个预训练的 Transformer 模型，用于各种 NLP 任务的微调并获得最先进的结果\n",
    "- 2018 年 10 月：[BERT](https://arxiv.org/abs/1810.04805)，另一个大型预训练模型，该模型旨在生成更好的句子摘要（下一章将详细介绍！）\n",
    "- 2019 年 2 月：[GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)，GPT 的改进（和更大）版本，由于道德问题未立即公开发布\n",
    "- 2019 年 10 月：[DistilBERT](https://arxiv.org/abs/1910.01108)，BERT 的蒸馏版本，速度提高 60%，内存减轻 40%，但仍保留 BERT 97% 的性能\n",
    "- 2019 年 10 月：[BART](https://arxiv.org/abs/1910.13461) 和 [T5](https://arxiv.org/abs/1910.10683)，两个使用与原始 Transformer 模型相同架构的大型预训练模型（第一个这样做）\n",
    "- 2020 年 5 月，[GPT-3](https://arxiv.org/abs/2005.14165)，GPT-2 的更大版本，无需微调即可在各种任务上表现良好（称为零样本学习zero-shot learning）\n",
    "\n",
    "这个列表并不全，只是为了突出一些不同类型的 Transformer 模型。大体上，它们可以分为三类：\n",
    "\n",
    "- GPT类（只使用transformer-decoder部分，自回归 Transformer 模型）\n",
    "- BERT类（只使用transformer-encoder部分，自编码 Transformer 模型）\n",
    "- BART/T5 类（Transformer-encoder-decoder模型）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0991f9d9",
   "metadata": {},
   "source": [
    "### Architectures和checkpoints\n",
    "对Transformer模型的研究中，会出现一些术语：架构Architecture和检查点checkpoint以及Model。 这些术语的含义略有不同：\n",
    "\n",
    "Architecture：定义了模型的基本结构和基本运算\n",
    "\n",
    "checkpoint：模型的某个训练状态，加载此checkpoint会加载此时的权重。（训练时可以选择自动保存checkpoint）\n",
    "\n",
    "Model:这是一个总称，不像“架构”或“检查点”那样精确，它可以同时表示两者。 当需要减少歧义时，本课程将指定架构或检查点。<br>\n",
    "例如，BERT 是一种 Architectures，而 bert-base-cased（谷歌团队为 BERT 的第一个版本训练的一组权重）是一个checkpoints。 但是，可以说“the BERT model”和“the bert-base-cased model”。\n",
    "\n",
    "***checkpoint概念在大数据里面说的比较多。模型在训练时可以设置自动保存于某个时间点（比如模型训练了一轮epoch，更新了参数，将这个状态的模型保存下来，为一个checkpoint。）\n",
    "所以每个checkpoint对应模型的一个状态，一组权重。大数据中检查点是一个数据库事件，存在的根本意义是减少崩溃时间。即减少因为意外情况数据库崩溃后重新恢复的时间。***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842779d3",
   "metadata": {},
   "source": [
    "### The Inference API\n",
    "[Model Hub](https://huggingface.co/models)（模型中心）包含多语言模型的checkpoints。您可以通过单击语言标签来优化对模型的搜索，然后选择生成另一种语言文本的模型。 \n",
    "\n",
    "通过单击选择模型后，您会看到有一个小部件——Inference API（支持在线试用）。即您可以直接在此页面上使用各种模型，通过输入自定义文本就可以看到模型处理输入数据后的结果。 通过这种方式，您可以在下载模型之前快速测试模型的功能。\n",
    "![DistilBERT base model (uncased)](https://img-blog.csdnimg.cn/0edebca3ab8248b4b2bac88f88ab79c0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa220c87",
   "metadata": {},
   "source": [
    "## 2. 用pipeline处理NLP问题\n",
    "在本节中，我们将看看 Transformer 模型可以做什么，并使用 🤗 Transformers 库中的第一个工具：管道pipeline。\n",
    "\n",
    ">🤗 [Transformers 库](https://github.com/huggingface/transformers)提供了创建和使用共享模型的功能.。[Model Hub](https://huggingface.co/models)包含数千个所有人都可以下载和使用的预训练模型。 您也可以将自己的模型上传到 Hub！\n",
    "\n",
    "🤗 Transformers 库中最基本的对象是pipeline。 它将模型与其必要的预处理和后处理步骤连接起来，使我们能够直接输入任何文本并获得可理解的答案：\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "```\n",
    "```python\n",
    "[{'label': 'POSITIVE', 'score': 0.9598047137260437}]\n",
    "```\n",
    "我们甚至可以传入几个句子！\n",
    "```python\n",
    "classifier([\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"I hate this so much!\"\n",
    "])\n",
    "```\n",
    "```python\n",
    "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
    " {'label': 'NEGATIVE', 'score': 0.9994558095932007}]\n",
    " ```\n",
    "默认情况下，此管道选择一个特定的预训练模型，该模型已针对英语情感分析进行了微调。 创建分类器对象时，将下载并缓存模型。 如果您重新运行该命令，则将使用缓存的模型，无需再次下载模型。\n",
    "\n",
    "将一些文本传递到管道时涉及三个主要步骤：\n",
    "\n",
    "1. 预处理：文本被预处理为模型可以理解的格式。\n",
    "2. 输入模型：构建模型，并将预处理的输入传递给模型。\n",
    "3. 后处理：模型的预测是经过后处理的，因此您可以理解它们。\n",
    "\n",
    "目前可用的一些管道是：\n",
    "- feature-extraction (获取文本的向量表示)\n",
    "- fill-mask填充给定文本中的空白（完形填空）\n",
    "- ner (named entity recognition)词性标注\n",
    "- question-answering问答\n",
    "- sentiment-analysis情感分析\n",
    "- summarization摘要生成\n",
    "- text-generation文本生成\n",
    "- translation翻译\n",
    "- zero-shot-classification零样本分类\n",
    "\n",
    "您也可以从 Hub 中针对特定任务来选择特定模型的管道 例如，文本生成。转到 [Model Hub](https://huggingface.co/models)并单击左侧的相应标签，页面将会仅显示文本生成任务支持的模型。\n",
    "(***除了模型要匹配任务，更进一步考虑的因素之一是：预训练模型训练时使用的数据集，要尽可能的接近你需要处理的任务所包含的数据集，两个数据集越接近越好。***） \n",
    "\n",
    "Transformers pipeline API 可以处理不同的 NLP 任务。您可以使用完整架构，也可以仅使用编码器或解码器，具体取决于您要解决的任务类型。 下表总结了这一点：\n",
    "\n",
    " 模型    | 例子 | 任务\n",
    "-------- | ----- |----- \n",
    "Encoder  | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa |句子分类、命名实体识别、抽取式问答\n",
    "Decoder  | CTRL, GPT, GPT-2, Transformer XL |文本生成\n",
    "Encoder-decoder  | BART, T5, Marian, mBART |摘要生成、翻译、生成式问答\n",
    "\n",
    "以上显示的pipeline主要用于演示目的。 它们是为特定任务编程的，不能执行它们的变体。 在下一节中，您将了解管道内部的内容以及如何自定义其行为。\n",
    ">上面这几种管道的简单示例可以查看——[Hugging Face主页课程第一篇《Transformer models》](https://blog.csdn.net/qq_56591814/article/details/120124306)。\n",
    ">或单击[Open in Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter1/section3.ipynb)以打开包含其它管道应用代码示例的 Google Colab 笔记本。\n",
    "如果您想在本地运行示例，我们建议您查看[设置](https://huggingface.co/course/chapter0)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaed469",
   "metadata": {},
   "source": [
    "## 3. Behind the pipeline\n",
    ">本节代码:[Open in Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section2_pt.ipynb) (PyTorch)<br>\n",
    ">YouTube视频：[what happend inside the pipeline function](https://youtu.be/1pedAIvTWXk)\n",
    "\n",
    "让我们从一个完整的例子开始，看看当我们在第1节中执行以下代码时，幕后发生了什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b855a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598048329353333},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558095932007}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier([\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"I hate this so much!\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b53f5",
   "metadata": {},
   "source": [
    "正如我们在第 1 章中看到的，这个管道将三个步骤组合在一起：预处理、通过模型传递输入和后处理：\n",
    "![full_nlp_pipeline ](https://img-blog.csdnimg.cn/7f19b775bfe94fa0bb9e35d883567e16.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)\n",
    "\n",
    "让我们快速浏览一下这些内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fcc38d",
   "metadata": {},
   "source": [
    "### tokenizer预处理\n",
    "与其他神经网络一样，Transformer 模型不能直接处理原始文本，因此我们管道的第一步是将文本输入转换为模型可以理解的数字。为此，我们使用了一个分词器tokenizer，它将负责：\n",
    "\n",
    "- 将输入拆分为称为标记的单词、子词subword或符号symbols（如标点符号）\n",
    "- 将每个标记映射到一个整数\n",
    "- 添加可能对模型有用的其他输入\n",
    "\n",
    "使用 AutoTokenizer 类及其 from_pretrained 方法，以保证所有这些预处理都以与模型预训练时完全相同的方式完成。设定模型的 checkpoint名称，它会自动获取与模型的Tokenizer关联的数据并缓存它（所以它只在你第一次运行下面的代码时下载）。\n",
    "\n",
    "由于情感分析管道的默认检查点是 [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)，我们可以运行以下命令得到我们需要的tokenizer："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07963015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11bc2dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#return_tensors=\"pt\"表示返回Pytorch张量。文本转换为数字之后必须再转换成张量tensors才能输入模型。\n",
    "#padding=True表示填充输入序列到最大长度，truncation=True表示过长序列被截断\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ade14",
   "metadata": {},
   "source": [
    "### 选择模型\n",
    "我们可以像使用分词器一样下载我们的预训练模型。 🤗 Transformers 提供了一个 AutoModel 类，它也有一个 from_pretrained 方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad74b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71608d25",
   "metadata": {},
   "source": [
    "AutoModel 类及其所有相关类实际上是库中各种可用模型的简单包装器。 它可以自动为您的checkpoint猜测合适的模型架构，然后使用该架构实例化模型。（***即AutoModel 类可以从checkpoint实例化任何模型，而且这是一种更好的实例化模型方法。构建模型还有另一种方法，放在文末。***）\n",
    "\n",
    "在此代码片段中，我们下载了之前在管道中使用的相同checkpoint（它实际上应该已经被缓存）并用它实例化了一个模型。但是这个架构只包含基本的 Transformer 模块：给定一些输入，它输出我们称之为隐藏状态hidden states的东西。虽然这些隐藏状态本身就很有用，但它们通常是模型另一部分（model head）的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4abea6",
   "metadata": {},
   "source": [
    "### Model heads\n",
    "我们可以使用相同的模型体系结构执行不同的任务，但是每个任务都有与之关联的不同的Model heads。\n",
    "\n",
    "Model heads:将隐藏状态的高维向量（也就是logits向量）作为输入，并将它们投影到不同的维度上。 它们通常由一个或几个线性层组成：\n",
    "![transformer_and_head](https://img-blog.csdnimg.cn/9156c4e09d184e7c88732e60ae59e05e.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA56We5rSb5Y2O,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)\n",
    "在此图中，模型由其embeddings layer和后续层表示。输入数据经过embeddings layer输出logits向量以产生句子的最终表示。\n",
    "\n",
    "🤗 Transformers 中有许多不同的架构可用，每一种架构都围绕着处理特定任务而设计。 下面列举了部分Model heads：\n",
    "* Model (retrieve the hidden states)\n",
    "* ForCausalLM\n",
    "* ForMaskedLM\n",
    "* ForMultipleChoice\n",
    "* ForQuestionAnswering\n",
    "* ForSequenceClassification\n",
    "* ForTokenClassification\n",
    "* and others 🤗\n",
    "\n",
    "以情感分类为例，我们需要一个带有序列分类的Model head（能够将句子分类为正面或负面）。 因此，我们实际上不会使用 AutoModel 类，而是使用 AutoModelForSequenceClassification：\n",
    "\n",
    "（***也就是说前面写的model = AutoModel.from_pretrained(checkpoint)并不能得到情感分类任务的结果，因为没有加载Model head***）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b569ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28740d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
       "          2607,  2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71e0e9",
   "metadata": {},
   "source": [
    "model head将我们之前看到的高维向量作为输入，并输出包含两个值（每个标签一个）的向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e54b2e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de60b188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
       "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5cc94",
   "metadata": {},
   "source": [
    "由于我们只有两个句子和两个标签，因此我们从模型中得到的结果是 2 x 2 的形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a98c0",
   "metadata": {},
   "source": [
    "### Post-processing后处理\n",
    "我们从模型中获得的作为输出的值本身并不一定有意义。 让我们来看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ed37d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6be292",
   "metadata": {},
   "source": [
    "我们的模型预测了第一个句子结果 [-1.5607, 1.6123] 和第二个句子的结果 [4.1692, -3.3464]。 这些不是概率，而是 logits，即模型最后一层输出的原始非标准化分数。 要转换为概率，它们需要经过一个 SoftMax 层。所有🤗 Transformers 模型都输出 logits，这是因为训练的损失函数一般会将最后一个激活函数（比如SoftMax）和实际的交叉熵损失函数相融合。<br>\n",
    "（***补充：在Pytorch里面，交叉熵损失CEloss不是数学上的交叉熵损失（NLLLoss）。Pytorch的CrossEntropyLoss就是把Softmax–Log–NLLLoss合并成一步。详细内容可以参考知乎文章[《如何理解NLLLoss?》](https://zhuanlan.zhihu.com/p/30187567)***）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "971038f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903159a3",
   "metadata": {},
   "source": [
    "这次输出是可识别的概率分数。\n",
    "\n",
    "要获得每个位置对应的标签，我们可以检查模型配置的 id2label 属性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "902d866c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d7edf7",
   "metadata": {},
   "source": [
    "现在我们可以得出结论，该模型预测了以下内容：\n",
    "\n",
    "第一句：NEGATIVE：0.0402，POSITIVE：0.9598<br>\n",
    "第二句：NEGATIVE：0.9995，POSITIVE：0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1a9f2",
   "metadata": {},
   "source": [
    "## 4. 构建Trainer API微调预训练模型\n",
    ">本节代码：[Open in Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section3.ipynb)（PyTorch），建议点此进行测试。colab上加载数据集非常快，设置GPU后训练也比较快。\n",
    ">打开后选择左上方“修改”选项卡，点击笔记本设置-硬件加速器None改成GPU就行。\n",
    "\n",
    "在第3节中，我们探讨了如何使用分词器和预训练模型进行预测。 但是，如果您想为自己的数据集微调预训练模型怎么办？ 这就是本章的主题！ 你将学习：\n",
    "\n",
    "- 如何从Model Hub 准备大型数据集\n",
    "- 如何使用high-level Trainer API来微调模型\n",
    "- 如何使用自定义训练循环a custom training loop\n",
    "- 如何利用 🤗 Accelerate 库在任何分布式设置上轻松运行该custom training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987f3cbf",
   "metadata": {},
   "source": [
    "### 从Hub上下载dataset\n",
    ">Youtube 视频：[Hugging Face Datasets Overview](https://youtu.be/_BZearw7f0w)（pytorch）\n",
    "\n",
    "Hub 不仅包含模型；还含有多个[datasets](https://huggingface.co/datasets)，这些datasets有很多不同的语言。我们建议您在完成本节后尝试加载和处理新数据集（[参考文档](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub)）。 \n",
    "\n",
    " MRPC 数据集是构成 [GLUE 基准](https://gluebenchmark.com/)的 10 个数据集之一。而GLUE 基准是一种学术基准，用于衡量 ML 模型在 10 个不同文本分类任务中的性能。\n",
    "\n",
    "🤗 Datasets库提供了一个非常简单的命令来下载和缓存Hub上的dataset。 我们可以像这样下载 MRPC 数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d06a156",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64030e57f17a47eebfe69d6f03539690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=7777.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fad1c6baf0f457695b16ef505be12dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=4473.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to C:\\Users\\Administrator\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273effe22d4f41afb0066922d8a10361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Downloading', layout=Layout(width='20px…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fcfb381cbeb48dab962a58065d0e634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Downloading', layout=Layout(width='20px…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d42b5d9ed754d1aab22eb3e6731f8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Downloading', layout=Layout(width='20px…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to C:\\Users\\Administrator\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab9929",
   "metadata": {},
   "source": [
    "**报错： ValueError: check_hostname requires server_hostname  的解决方法**\n",
    "\n",
    "pip install urllib3==1.25.11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a471a",
   "metadata": {},
   "source": [
    "这样就得到一个DatasetDict对象，包含训练集、验证集和测试集，训练集中有3,668 个句子对，验证集中有408对，测试集中有1,725 对。每个句子对包含四列数据：'sentence1', 'sentence2', 'label'和 'idx'。\n",
    "\n",
    "load_dataset 方法, 可以从不同的地方构建数据集<br>\n",
    "- from the HuggingFace Hub,\n",
    "- from local files, 如CSV/JSON/text/pandas files\n",
    "- from in-memory data like python dict or a pandas dataframe.\n",
    "\n",
    "例如： datasets = load_dataset(\"text\", data_files={\"train\": path_to_train.txt, \"validation\": path_to_validation.txt} 具体可以[参考文档](https://link.zhihu.com/?target=https%3A//huggingface.co/docs/datasets/loading_datasets.html%23from-local-files)\n",
    "\n",
    "load_dataset命令下载并缓存数据集，默认在 ~/.cache/huggingface/dataset 中。您可以通过设置 HF_HOME 环境变量来自定义缓存文件夹。\n",
    "\n",
    "和字典一样，raw_datasets 可以通过索引访问其中的句子对："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d749fddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'label': 1,\n",
       " 'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b1c4011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>He said the foodservice pie business doesn 't ...</td>\n",
       "      <td>\" The foodservice pie business does not fit ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>Magnarelli said Racicot hated the Iraqi regime...</td>\n",
       "      <td>His wife said he was \" 100 percent behind Geor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>The dollar was at 116.92 yen against the yen ,...</td>\n",
       "      <td>The dollar was at 116.78 yen JPY = , virtually...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>The AFL-CIO is waiting until October to decide...</td>\n",
       "      <td>The AFL-CIO announced Wednesday that it will d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>No dates have been set for the civil or the cr...</td>\n",
       "      <td>No dates have been set for the criminal or civ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>4023</td>\n",
       "      <td>0</td>\n",
       "      <td>Their contract will expire at 12 : 01 a.m. Wed...</td>\n",
       "      <td>\" It has outraged the membership , \" said Rian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>4028</td>\n",
       "      <td>1</td>\n",
       "      <td>But plaque volume increased by 2.7 percent in ...</td>\n",
       "      <td>The volume of plaque in Pravachol patients ' a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>4040</td>\n",
       "      <td>1</td>\n",
       "      <td>Today in the US , the book - kept under wraps ...</td>\n",
       "      <td>Tomorrow the book , kept under wraps by G. P. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>4049</td>\n",
       "      <td>0</td>\n",
       "      <td>The S &amp; P / TSX composite rose 87.74 points on...</td>\n",
       "      <td>On the week , the Dow Jones industrial average...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>4053</td>\n",
       "      <td>1</td>\n",
       "      <td>Ex-KGB agent Putin added that the Beatles were...</td>\n",
       "      <td>In Soviet times the Beatles ' music \" was cons...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>408 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      idx  label                                          sentence1  \\\n",
       "0       9      1  He said the foodservice pie business doesn 't ...   \n",
       "1      18      0  Magnarelli said Racicot hated the Iraqi regime...   \n",
       "2      25      0  The dollar was at 116.92 yen against the yen ,...   \n",
       "3      32      1  The AFL-CIO is waiting until October to decide...   \n",
       "4      33      0  No dates have been set for the civil or the cr...   \n",
       "..    ...    ...                                                ...   \n",
       "403  4023      0  Their contract will expire at 12 : 01 a.m. Wed...   \n",
       "404  4028      1  But plaque volume increased by 2.7 percent in ...   \n",
       "405  4040      1  Today in the US , the book - kept under wraps ...   \n",
       "406  4049      0  The S & P / TSX composite rose 87.74 points on...   \n",
       "407  4053      1  Ex-KGB agent Putin added that the Beatles were...   \n",
       "\n",
       "                                             sentence2  \n",
       "0    \" The foodservice pie business does not fit ou...  \n",
       "1    His wife said he was \" 100 percent behind Geor...  \n",
       "2    The dollar was at 116.78 yen JPY = , virtually...  \n",
       "3    The AFL-CIO announced Wednesday that it will d...  \n",
       "4    No dates have been set for the criminal or civ...  \n",
       "..                                                 ...  \n",
       "403  \" It has outraged the membership , \" said Rian...  \n",
       "404  The volume of plaque in Pravachol patients ' a...  \n",
       "405  Tomorrow the book , kept under wraps by G. P. ...  \n",
       "406  On the week , the Dow Jones industrial average...  \n",
       "407  In Soviet times the Beatles ' music \" was cons...  \n",
       "\n",
       "[408 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "validation=pd.DataFrame(raw_datasets['validation'])\n",
    "validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89139602",
   "metadata": {},
   "source": [
    "可见标签已经是整数，不需要再做任何预处理。通过raw_train_dataset的features属性可以知道每一列的类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "debb009c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c73e8d",
   "metadata": {},
   "source": [
    "label是 ClassLabel 类型，label=1表示这对句子互为paraphrases，label=0表示句子对意思不一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244e813",
   "metadata": {},
   "source": [
    "### 数据集预处理\n",
    ">YouTube视频[《Preprocessing sentence pairs》](https://youtu.be/0u3ioSwev3s)\n",
    "\n",
    "通过tokenizer可以将文本转换为模型能理解的数字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10bcde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72188c34",
   "metadata": {},
   "source": [
    "让我们看一个示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0747c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8d07d",
   "metadata": {},
   "source": [
    "所以将句子对列表传给tokenizer，就可以对整个数据集进行分词处理。因此，预处理训练数据集的一种方法是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f03a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509983f",
   "metadata": {},
   "source": [
    "这种处理方法是ok的，但缺点是处理之后tokenized_dataset不再是一个dataset格式，而是返回字典（带有我们的键:input_ids、attention_mask 和 token_type_ids，对应的键值对的值）。 而且一旦我们的dataset过大，无法放在内存中，那么这样子的做法会导致 Out of Memory 的异常。（ 🤗 Datasets 库中的数据集是存储在磁盘上的 Apache Arrow 文件，因此请求加载的样本都保存在内存中）。\n",
    "\n",
    "为了使我们的数据保持dataset的格式，我们将使用更灵活的Dataset.map 方法。此方法可以完成更多的预处理而不仅仅是 tokenization。 map 方法是对数据集中的每个元素应用同一个函数，所以让我们定义一个函数来对输入进行tokenize预处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "793dceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5314a7a",
   "metadata": {},
   "source": [
    "这个函数接受的是一个字典（就像我们dataset的items），返回的也是一个字典（有三个键：input_ids、attention_mask 和 token_type_ids ）。 \n",
    "\n",
    "在tokenization函数中省略了padding 参数，这是因为padding到该批次中的最大长度时的效率，会高于所有序列都padding到整个数据集的最大序列长度。 当输入序列长度很不一致时，这可以节省大量时间和处理能力！\n",
    "\n",
    "以下是对整个数据集应用tokenization方法。 我们在 map 调用中使用了 batched=True，因此该函数一次应用于数据集的整个batch元素，而不是分别应用于每个元素。 这样预处理速度会更快（因为 🤗 Tokenizers 库中的Tokenizer用 Rust 编写，一次处理很多输入时这个分词器可以非常快）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11da7eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d311a1e6536742e69b412b1acaaf8864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d3b14eb47845828e1acdcc1a54f8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9222dc46474a0ea98231ee3cbeda9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d3d92",
   "metadata": {},
   "source": [
    ">如果您没有使用由该库支持的fast tokenizer，Dataset.map函数进行预处理时可以设定num_proc 参数来进行多线程处理，加快预处理速度。\n",
    "\n",
    "最后，当我们将输入序列进行批处理时，要将所有输入序列填充到本批次最长序列的长度——我们称之为动态填充技术dynamic padding(动态填充：即将每个批次的输入序列填充到一样的长度。具体内容放在最后）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0a5a4c",
   "metadata": {},
   "source": [
    "###  使用 Trainer API 在 PyTorch 中进行微调\n",
    "由于 PyTorch 不提供封装好的训练循环，🤗 Transformers 库写了了一个transformers.Trainer API，它是一个简单但功能完整的 PyTorch 训练和评估循环，针对 🤗 Transformers 进行了优化，有很多的训练选项和内置功能，同时也支持多GPU/TPU分布式训练和混合精度。即Trainer API是一个封装好的训练器（Transformers库内置的小框架，如果是Tensorflow，则是TFTrainer）。\n",
    "\n",
    "数据预处理完成后，只需要几个简单的步骤来定义Trainer的参数，就可以使用 Trainer 进行模型的基本训练循环了（否则的话，要自己从头加载和预处理数据，设置各种参数，一步步编写训练循环。自定义训练循环的内容在本节最后）。 \n",
    "\n",
    "Trainer最困难的部分可能是准备运行 Trainer.train 的环境，因为它在 CPU 上运行速度非常慢。（ 如果您没有设置 GPU，则可以在 Google Colab 上访问免费的 GPU 或 TPU）\n",
    "\n",
    "trainer主要参数包括：\n",
    "- Model：用于训练、评估或用于预测的模型\n",
    "- args (TrainingArguments）：训练调整的参数。如果未提供，将默认为 TrainingArguments 的基本实例\n",
    "- data_collator（DataCollator，可选）– 用于批处理train_dataset 或 eval_dataset 的的函数\n",
    "- train_dataset：训练集\n",
    "- eval_dataset：验证集\n",
    "- compute_metrics：用于计算评估指标的函数。必须传入EvalPrediction 并将返回一个字典，键值对是metric和其value。\n",
    "- callbacks （回调函数，可选）：用于自定义训练循环的回调列表（List of TrainerCallback）\n",
    "- optimizers：一个包含优化器和学习率调整器的元组，默认优化器是AdamW，默认的学习率是线性的学习率，从5e-5 到 0\n",
    "\n",
    "\n",
    "除了以上主要参数还有一些参数和属性（得有几十个吧，可以慢慢看。完整的Trainer文档可以参考[这里](https://huggingface.co/transformers/main_classes/trainer.html?highlight=trainer#transformers.Trainer)）\n",
    "\n",
    "下面的代码示例假定您已经执行了上一节中的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13f3b71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Administrator\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db34e0abde95433f9c991d93b97ae0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca002bb474f642d4bd44446f57256732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71ee820d7a745be9768507ce64dfb98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")#MRPC判断两个句子是否互为paraphrases\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)#动态填充，即将每个批次的输入序列填充到一样的长度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f470a3",
   "metadata": {},
   "source": [
    "#### 训练\n",
    "Trainer 第一个参数是TrainingArguments类，是一个与训练循环本身相关的参数的子集，包含 Trainer中用于训练和评估的所有超参数。 唯一一个必须提供的参数是：保存model或者说是checkpoint的目录，其它参数可以选取默认值（比如默认训练3个epoch等）（TrainingArguments也有几十个参数，，常见参数写在文末，完整文档包含在上面说的Trainer文档里）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83b72087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adffa1e7",
   "metadata": {},
   "source": [
    "第二步：定义模型\n",
    "和上一节一样，我们将使用 AutoModelForSequenceClassification 类，带有两个标签：\n",
    "（***其实就是根据自己的任务选择任务头task head，以便进行微调***）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "222e5932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)#标签数为2也就是二分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3cedda",
   "metadata": {},
   "source": [
    "在实例化此预训练模型后会报一个warning。 这是因为 BERT 没有在句子对分类方面进行过预训练，所以预训练模型的head已经被丢弃，而是添加了一个适合序列分类的new head。 警告表明一些权重没有使用（对应于丢弃的预训练head部分），而其他一些权重被随机初始化（new head部分）， 最后鼓励您训练模型。\n",
    "\n",
    "有了模型之后，就可以定义一个训练器Trainer，将迄今为止构建的所有对象传递给它进行模型精调。这些对象包括：model、training_args、训练和验证数据集、data_collator 和tokenizer。（这都是Trainer的参数）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9a03100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8246ae",
   "metadata": {},
   "source": [
    "像上面这样传递tokenizer时，参数data_collator 是之前定义的动态填充DataCollatorWithPadding，所以此调用中的 data_collator=data_collator行可以跳过。（但是像之前一样写出这一步很重要It was still important to show you this part of the processing in section 2!）\n",
    "\n",
    "要在我们的数据集上微调模型，我们只需要调用 Trainer 的 train方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cc93ced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 1.14 GiB already allocated; 3.30 MiB free; 1.21 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_520/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\nlp_with_transformers\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[0;32m   1093\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1095\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\nlp_with_transformers\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\nlp_with_transformers\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\nlp_with_transformers\\lib\\site-packages\\transformers\\optimization.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    334\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"exp_avg\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m                     \u001b[1;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m                     \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"exp_avg_sq\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg_sq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"exp_avg\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"exp_avg_sq\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 1.14 GiB already allocated; 3.30 MiB free; 1.21 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d818d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_with_transformers]",
   "language": "python",
   "name": "conda-env-nlp_with_transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
